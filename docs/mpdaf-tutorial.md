## MPDAF Tutorial

**Author: P. Shrestha (2024)**


### Introduction
The Multi-Physics Data Assimilation Framework (MPDAF) is a lite software infrastructure for conducting data assimilation 
experiments with research orinted physics model using NCAR DART. The tutorial here is designed to make use of the 
snow hydrology model (MSHM) for a 1-D test case over NASA SnowEx site in GrandMesa. 

### Requirements
The tutorial is designed such that any new user can run it without prior knoweldge of the system, but the user would benefit with knowledge of the model, DA system and basic idea of working on linux systems (e.g., shell scripting and job schedulers). The tutorial makes use of the following model and DA system:
* [MultiLayer Snow Hydrology Model (MSHM)](https://doi.org/10.3390/rs12203422)
* [Data Assimilation Research Testbed (DART)](https://dart.ucar.edu)

### Snow Data Assimilation

Prognostic states in snow hydrology model have spatially and temporarily variable depths, which
is a technical challenge in assimilating observed variables at different depths. However, diagnostic
variables like total snow depth and total snow water equivalent (SWE) are also the variables which are generally
retrieved from remote sensing measurements along with pit observations. So, the general idea is to
assimilate these two variables and redistribute the increments vertically into the the prognostic
model states.

### DART interface in MPDAF

The **dart** folder contains three subfolders:

* mshm - coupling interface of model with dart
* shellscripts - scripts for syncing source codes,  running model, dart and automation
* tools - interface for processing real observations and converting to DART states

The model restart files are imporant for the offline data assimilation using DART.
The model stops at the time of assimilaton, and DART reads the model states
and updates the restart file after assimilation. So, we need to have model consistency in reproducing the same 
result with the restart files as for the continous runs. This has been achieved using the consistent precision
in the codes and multiple tests with restart files and comparing with continuous runs. Additionally, the above
two diagnostic variables (total snow depth and total SWE) have also been added in the restart files for assimilation purpose.

For DA we need to quantify the model errors, so ensemble run has been added as option in MPDAF, and currently the
ensemble is generated by perturbing precipitation,temperature and radiation forcing, whose amplitude can be specified in the setup namelist. DART also has its own namelist for choosing filters, inflation algorithm, cutoff radius etc. For this particular test case, we use the Ensemble Adjustment Kalman Filter (EAKF) and enhanced spatially varying inflation algorithm. 

The DART run contains the following main steps
 * model data conversion to DART prior states
 * assimilation of observation using selected filter and inflation algorithm
 * DART posterior states conversion to new model restart files

### Getting Started

First, the user will need to get an account and login to the CCI cluster at UIUC.
Then, login into the CCI cluster and clone MPDAF from the git repository

* cd
* git clone https://github.com/mpdaf23/mpdaf.git
* cd mpdaf 

**mpdaf** folder contains:

1. bld - main directory for model compilation, experiment setup.
2. bld/intf_DA - DART interface with physics based models
3. bld/setups -  experiment setups (e.g., SnowEx, GrandMesa etc.)
4. bld/machines - machine setups for compilation and job submission 
5. mshm - source code for MSHM.
6. tmp_mshm - temporary space to compile the MSHM source code.
7. fo - forward operators (e.g., radiative transfer codes)
8. bin - location for executable after model compilation.

### Model Compilation

* cd bld
* ./build.ksh 0  <for clean compilation>

Users should avoid changing the source code in mshm folder. Instead, one can update the source code stored in tmp_mshm folder and recompile the code using:

* ./build.ksh 1

### Experiment Setup (CTRL RUN)

Edit the **setup.ksh** script for NASA SnowEx test case and save.

>inputdatadir="/home/$USER/scratch/inputdata/" #Path of forcing files

>ibegin=0                              #Start time step 

>ntsp=17035                             #Total time step

>nlevs=30                              #Number of vertical levels

>ngridpts=1                            #Number of Grid Pts

>dt=1800                               #Time step in seconds 

>ptb1=0.4                              #Multiplicative random uniform Precipitation perturbation

>ptb2=2.0                              #Additive random uniform Temperature perturbation

>ptb3=0.05                             #Multiplicative random uniform SW perturbation

>ptb4=0.1                              #Multiplicative random uniform LW perturbation

>ptb5=0.25                             #Additive random uniform Kscale perturbation

>kfrac=1.0                             #Melt fraction of top layer for rain on snow

>fdate="2016 10 11 00 00 00"           #Start day of run (yyyy mm dd hh mm ss)

>platform="CCI"                        #Select Machine

>testcase="SnowEx"                     #Select Testcase

>numEns=1                              #Select ensemble size

>scratchdir="/scratch/users/${USER}/"  #Select rundir

The user will need to provide the "METEOROLOGICAL FORCING" data under the following folder name matching with the "testcase" in the above namelist (/scratch/users/${USER}/data/SnowEx) . The setup script will link the inputdatdir to this physical path.
The meteorological forcing from SnowEx testcase is available from [UIBOX](https://uofi.box.com/v/snowEx17forcingdata)

* ./setup.ksh 

### Model Run

* cd /scratch/users/$USER//mpdaf_CCI_run

* sbatch mpdaf_run.ksh

### Experiment Setup (OPEN LOOP RUN)

Check if the above run is complete (users can check using squeue -u $USER) - also check the outputs generated in the /scratch/users/$USER//mpdaf_CCI_run folder (It should contain model output (*.out) and restart files). Then, follow the steps for CTRL run by just changing **numEns=48** in the setup.ksh script. This will setup an ensemble experiment by perturbing the forcing using the parameters _ptbX_. Then submit the model run as above.

### Data Assimilation Experiments

Download DART into root directory.
* cd
* git clone https://github.com/NCAR/DART.git

It is possible that some routines in newver version of DART might not be compatible with MPDAF. The MPDAF interface for DART was built with DART v1.8.3.1, with last commit by Helen Kershaw on Wed May 10 13:06:50 2023 -0400 (commit 72e5740b71ef6c7cde4c6f946e2c1df2019365fb).In such case, one might need to change to older version.  

Install NETCDF libraray necessary for DART compilation. If not alreaedy in the system, the user will need to install zlib, hdf, netcdf-c and netcdf-fortran
If you are installing netcdf-fortran libraries locally, the following steps are useful to configure and install.

* CPPFLAGS='-I/path/to/netcdf-c/include -I/path/to/hdf5/include -I/path/to/zlib/include' 
* LDFLAGS='-L/path/to/netcdf-c/lib -L/ path/to/hdf5/lib -L/ path/to/zlib/lib' 
* ./configure - prefix=/$home/local
* make check
* make install

Specify NETCDF paths, one can specify these paths in their .bashrc file

* export NETCDF=/path/to/netcdf
* export NETCDFC=/path/to/netcdfc
* export LD_LIBRARY_PATH=${NETCDF}/lib:${LD_LIBRARY_PATH}
* export PATH=${NETCDF}/bin:${PATH}

Update the DART mkmf.template

* cd DART/build_templates/
* Edit mkmf.template
 * comment out the NETCDF path  #NETCDF=/usr/local
 * INCS = -I$(NETCDF)/include -I$(NETCDFC)/include
 * LIBS = -L$(NETCDF)/lib -L$(NETCDFC)/lib -lnetcdff -lnetcdf

Now move to MPDAF to interface MSHM with DART,  which is used to build, compile and run DA experiments
and update/compile DART model interface

The DART namelist "input.nml" is located in $HOME/mpdaf/bld/intf_DA/dart/mshm/work.
Edit $HOME/mpdaf/bld/intf_DA/dart/mshm/work/input.nml and change "mshm_geo_file" path to the correct geolocation file in the SnowEx forcing directory (see above).

* cd
* cd mpdaf/bld/intf_DA/dart/shellscripts/
* ./git_to_dart_mshm.csh
* cd $USER/DART/models/mshm/work
* ./quickbuild.sh

Update and compile DART observation operator

* cd
* cd mpdaf/bld/intf_DA/dart/shellscripts/
* ./git_to_dart_obs.csh
* cd $USER/DART/observations/obs_converters/text_snex/work
* ./quickbuild.sh

### Observation data
Observations are available in different formats, here we used SnowEX text data and convert it into DART observation
space (obs_seq.out). The time of observations in this file controls the length of execution of DART filter, so we
only created DART observation files containing one time stamp. During DART run, specific observation files are linked
based on the time stamp generated in dart_assim_time.txt which is created while DART runs.

Currently there are two observation types enabled for data assimilation.

1. SNOW_DEPTH
2. SNOW_WATER_EQUIVALENT

Generate observations for assimilation in DART (obs.seq files)

* cd $USER /DART/observations/obs_converters/text_snex/data
* mkdir snex_test
* cd snex_test
* vi data_snex_yyyymmddhhmnss 

Enter your ascii observation

>VarType Latitude Longitude Height yyyy mm dd hh mn ss Obs Error

>VarType = 1 (Total Snow Depth), 2 (Total SWE)

>Latitute/Longitude in degrees, longitude spans (0 to 360)

>Height = 0.  (Surface, always)

>Obs. = [m]

>Error = [m] e.g., 10%, 20% of Obs. (includes instrument + representative error)

* cd $USER/mpdaf/bld/intf_DA/dart/shellscripts

Edit snex_to_dart.ksh file and choose which data to be used.
If you have multiple data, the script will process all files to generate the respective DART space observation.

Choose which data to be used and create DART observation space data.
 * cp snex_test/data_snex* . (Copy it from snex_test directory that you created above)
 * ./snex_to_dart.ksh

** The script below is for sensitivity study, where you remove observation at intervals and is not needed for this exercise.
If the observations are not daily, one needs to run the script below to the desired frequency (e.g., Nfreq = 2, 3, 5 )**

* ./cropper_snex.sh Nfreq

### Experiment Setup (ENSEMBLE DA RUN)

Setup and make initial ensemble run to the time, when the first observation is available for assimilation. 
For this, set **ntstp** accordingly in the setup script- Edit setup.ksh

* ./setup.ksh 
* cd /scratch/users/$USER//mpdaf_CCI_run
* sbatch mpdaf_run.ksh
* cd

Now, we need to update the namelist for the ensemble runs in **scratch** directory and start the assimilation runs. 
Edit mpdaf.ksh in DART shellscripts directory to update the istart (start step of model run) and 
itime (number of time steps to run the model). So, istart should be equal to the number of time step that 
you make the initial first run, and itime should be equal to the number of time step you want to run, 
usually till the next observation is available or user dependent (if you want to restart runs at regular interval, or continue till the end of forcing data).
This will update namelist for all ensemble members.

* cd mpdaf/bld/intf_DA/dart/shellscripts
* ./mpdaf.ksh Nens    <Nens=48 here>

Make first data assimilation

* sbatch dart_ini.ksh Nens

Check if DART has run (squeue -u $USER). It will also create log and error outputs in the current directory. Especially , check the ouput log file to see if DART ran successfully. 
This run will first convert the model restart files to "dart_prior" netcdf file in the run directory (/scratch/users/$USER//mpdaf_CCI_run), and create a "dart_assim_time" text file based on current restart file, which will be used to link the DART input observations. Then it will run the "filter" and produce "dart_posterior" netcdf file. Finally, these netcdf file will be converted to "model restart" file by overwriting the existing ones. 

And make the model run after the above step completes, so first check in the instances of mpdaf_CCI_run, if new restart files are generated in the ensemble instances. 
Eg., 
* ls /scratch/users/$USER//mpdaf_CCI_run/mpdaf_instances_10/* -ltr

Then make the restart run

* cd /scratch/users/$USER//mpdaf_CCI_run
* sbatch mpdaf_run.ksh


**The details below is for more experienced users !**
Execute a continuous DA run

* sbatch jobchain.ksh Nens run

For continuation run without assimilation, remove the observation files from DART/observations/obs_converters/work directory and simply run with

* sbatch continute.ksh Nens run

### Flowchart for DA experiment

* Compile code
* Setup run
* Make initiail spinup run
* Update namelist for DA assim and model restart frequency
* Sync DA and Obs Converter codes
* Prepare Obs data
* Submit DA jobscript
  * ./git_to_dart_mshm.csh
  * ./mpdaf.ksh NENS
  * sbatch dart_ini.ksh NENS
   * sbatch jobchain.ksh NENS run
